<!doctype html>
<html lang=" en-US">
  <head>
    <title>Yineng Zhang</title>

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta
      name="description"
      content="Yineng Zhang is a team member at LMSYS Org."
    />

    <meta
      name="keywords"
      content="Yineng Zhang | LMSYS Org | Computer Science | Machine Learning | Deep Learning | Artificial Intelligence | AI | Machine Learning System"
    />

    <link rel="canonical" href="https://zhyncs.com" />

    <link
      rel="icon"
      media="(prefers-color-scheme:dark)"
      href=""
      type="image/png"
    />
    <link
      rel="icon"
      media="(prefers-color-scheme:light)"
      href="assets/img/favicon.png"
      type="image/png"
    />
    <script
      src="./assets/js/favicon-switcher.js"
      type="application/javascript"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css"
      integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw="
      crossorigin="anonymous"
    />
    <script
      src="https://kit.fontawesome.com/a860a211d3.js"
      crossorigin="anonymous"
    ></script>

    <link rel="stylesheet" href="./assets/css/font_sans_serif.css" />

    <link rel="stylesheet" href="./assets/css/style-no-dark-mode.css" />
    <link rel="stylesheet" href="./assets/css/publications-no-dark-mode.css" />
  </head>

  <body>
    <div class="wrapper">
      <header>
        <a class="image avatar"
          ><img src="assets/img/avatar.jpg" alt="avatar"
        /></a>

        <h1 style="margin: 5pt">Yineng Zhang</h1>

        <email>me [at] zhyncs.com</email>

        <br />
        <br />
        <div class="social-icons">
          <a
            style="margin: 0 5px 0 0"
            href="https://scholar.google.com/citations?user=SmZ0KcYAAAAJ&hl=en"
          >
            <i class="ai ai-google-scholar" style="font-size: 1.5rem"></i>
          </a>

          <a style="margin: 0 5px 0 0" href="https://github.com/zhyncs">
            <i class="fa-brands fa-github" style="font-size: 1.5rem"></i>
          </a>

          <a
            style="margin: 0 5px 0 0"
            href="https://www.linkedin.com/in/zhyncs"
          >
            <i class="fa-brands fa-linkedin" style="font-size: 1.5rem"></i>
          </a>

          <a style="margin: 0 0 0 0" href="https://x.com/zhyncs42">
            <i class="fa-brands fa-x-twitter" style="font-size: 1.5rem"></i>
          </a>
        </div>
        <br />
      </header>
      <section>
        <h3 id="about-me">About Me</h3>

        <p>
          I'm a Principal AI Researcher at
          <a href="https://www.together.ai/">Together AI</a> and
          <strong>the creator and lead of TGL</strong>, the company’s
          proprietary inference engine. My journey with
          <a href="https://github.com/sgl-project/sglang">SGLang</a> has evolved
          from one of the first core developers, to leading
          <a href="https://github.com/sgl-project/sglang/issues/7736"
            >inference optimization efforts</a
          >, and eventually taking on a core maintainer role to support its next
          phase of growth. I <strong>initiated and led</strong> the end-to-end
          efforts from
          <a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"
            >day-0 support</a
          >
          and subsequent
          <a href="https://nebius.com/customer-stories/sglang"
            >performance optimization</a
          >
          for DeepSeek V3/R1, to
          <a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"
            >large-scale EP deployment</a
          >
          and
          <a href="https://lmsys.org/blog/2025-06-16-gb200-part-1/"
            >GB200 NVL72 integration</a
          >. While these were community collaborations, I drove the technical
          roadmap and ensured timely releases that placed our system ahead of
          other inference engines during that period. I am a selected member of
          the <a href="https://lmsys.org/about/">LMSYS Org</a> and I am also a
          committer to
          <a href="https://github.com/flashinfer-ai/flashinfer">FlashInfer</a>,
          and co-authored the
          <a href="https://arxiv.org/abs/2501.01005">FlashInfer paper</a> (
          <a href="https://mlsys.org/virtual/2025/poster/3259"
            >MLSys 2025 Best Paper</a
          >). Previously, I was a Lead Software Engineer at
          <a href="https://www.baseten.co/">Baseten</a>, where I co-authored the
          <a
            href="https://www.baseten.co/blog/private-secure-deepseek-r1-in-production-in-us-eu-data-centers/"
            >DeepSeek V3</a
          >
          and
          <a
            href="https://www.baseten.co/blog/day-zero-benchmarks-for-qwen-3-with-sglang-on-baseten/"
            >Qwen 3</a
          >
          launch blogs. Earlier, I worked at Meituan on
          <a href="https://tech.meituan.com/2022/03/03/ctr-gpu-inference.html"
            >CTR GPU inference</a
          >
          and
          <a
            href="https://tech.meituan.com/2024/04/11/gpu-vector-retrieval-system-practice.html"
            >vector retrieval</a
          >
          systems.
        </p>

        <h3 id="projects">Projects</h3>

        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://github.com/sgl-project/sglang"
            style="font-weight: 500"
            >SGLang</a
          >: SGLang is a fast serving framework for large language models and
          vision language models, which has been adopted by
          <em
            ><strong><a href="https://www.amd.com">AMD</a></strong></em
          >
          and
          <em
            ><strong><a href="https://x.ai">xAI</a></strong></em
          >.
          <div style="line-height: 50%"><br /></div>
          <a
            href="https://github.com/flashinfer-ai/flashinfer"
            style="font-weight: 500"
            >FlashInfer</a
          >: FlashInfer is a library and kernel generator for Large Language
          Models that provides high-performance implementation of LLM GPU
          kernels, which has been adopted by
          <em
            ><strong
              ><a href="https://github.com/sgl-project/sglang"
                >SGLang</a
              ></strong
            ></em
          >,
          <em
            ><strong
              ><a href="https://github.com/vllm-project/vllm">vLLM</a></strong
            ></em
          >
          and
          <em
            ><strong
              ><a href="https://github.com/mlc-ai/mlc-llm">MLC LLM</a></strong
            ></em
          >.
        </div>
        <div style="line-height: 100%"><br /></div>

        <h3 id="interviews">Interviews</h3>
        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://www.nytimes.com/2025/01/28/business/deepseek-owner-china-ai.html"
            style="font-weight: 500"
            ><em>The New York Times</em>: DeepSeek’s Rise: How a Chinese
            Start-Up Went From Stock Trader to A.I. Star</a
          >:
          <blockquote>
            “Most of the team graduated from the top universities in China,”
            said Yineng Zhang, a lead software engineer at Baseten in San
            Francisco who works on the SGLang, a project not part of DeepSeek
            that helps people build on top of DeepSeek’s system. “They are very
            smart and very young.”
          </blockquote>
        </div>
        <div style="line-height: 50%">
          <br />
        </div>
        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://www.nytimes.com/2025/01/23/technology/deepseek-china-ai-chips.html"
            style="font-weight: 500"
            ><em>The New York Times</em>: How Chinese A.I. Start-Up DeepSeek Is
            Competing With Silicon Valley Giants</a
          >:
          <blockquote>
            While employees at big Chinese technology companies are limited to
            collaborating with colleagues, “if you work on open source, you work
            with talent around the world,” said Yineng Zhang, lead software
            engineer at Baseten in San Francisco who works on the open source
            SGLang project. He helps other people and companies build products
            using DeepSeek’s system.
          </blockquote>
        </div>
        <div style="line-height: 50%">
          <br />
        </div>
        <div style="padding-right: 10px; padding-left: 20px">
          <a href="https://www.latent.space/p/baseten" style="font-weight: 500"
            ><em>Latent Space</em>: Everything you need to run Mission Critical
            Inference (ft. DeepSeek v3 + SGLang)</a
          >: Baseten's Amir Haghighat and Yineng Zhang on DeepSeek V3,
          quantization, pricing strategies, SGLang, open source AI, and the
          three pillars of Mission Critical Inference

          <div class="links">
            <a href="https://www.youtube.com/watch?v=KjH7Gl0_pq0">
              Latent Space Youtube
            </a>
          </div>
        </div>

        <div style="line-height: 100%"><br /></div>

        <h3 id="talks">Talks</h3>

        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://github.com/basetenlabs/SGLang-Workshop/blob/main/SGLang%20Workshop%20Slides.pdf"
            style="font-weight: 500"
            >Introduction to LLM serving with SGLang</a
          >: A hands-on session was delivered at the AI Engineer World's Fair
          2025.
        </div>

        <div style="line-height: 50%">
          <br />
        </div>

        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://github.com/sgl-project/sgl-learning-materials/blob/main/slides/cuda_tech_briefing_at_nvidia_gtc_2025.pdf"
            style="font-weight: 500"
            >CUDA Tech Briefing at NVIDIA GTC 2025</a
          >: A technical talk on SGLang was presented at NVIDIA GTC 2025,
          focusing on DeepSeek V3 optimization and the importance of CUDA JIT.
        </div>

        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://gamma.app/docs/SGLang-v04-Optimization-6x6pml7351oy58r?mode=doc"
            style="font-weight: 500"
            >SGLang v0.4 Optimization</a
          >: A technical talk on SGLang was delivered at
          <a
            href="https://camel-ai-24h-hackathon.devpost.com/"
            style="font-weight: 500"
            >CAMEL-AI Hackathon: Mastering Multi-Agent Systems</a
          >.
        </div>

        <div style="line-height: 50%">
          <br />
        </div>

        <div style="padding-right: 10px; padding-left: 20px">
          <a
            href="https://www.youtube.com/watch?v=XQylGyG7yp8"
            style="font-weight: 500"
            >SGLang Performance Optimization</a
          >: A technical talk on SGLang was delivered at GPU MODE, which
          represents the world's largest GPU developer community.

          <div class="links">
            <a href="https://github.com/gpu-mode/lectures/tree/main/lecture_035"
              >Slide</a
            >
          </div>
        </div>

        <div style="line-height: 100%"><br /></div>

        <h3 id="technical_blogs" style="margin: 2px 0px -15px">
          Technical Blogs
        </h3>

        <div class="publications">
          <ol class="bibliography">
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Deploying DeepSeek on GB200 NVL72 with PD and Large Scale EP
                    (Part I): 2.7x Higher Decoding Throughput
                  </div>
                  <div>Lead GB200 NVL72 project</div>
                  <div class="links">
                    <a href="https://lmsys.org/blog/2025-06-16-gb200-part-1/"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Deploying DeepSeek with PD Disaggregation and Large-scale
                    Expert Parallelism on 96 H100 GPUs
                  </div>
                  <div>Co lead optimization of DeepSeek V3/R1 on SGLang</div>
                  <div class="links">
                    <a href="https://lmsys.org/blog/2025-05-05-large-scale-ep"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Day zero benchmarks for Qwen 3 with SGLang on Baseten
                  </div>
                  <div class="author">
                    <strong>Yineng Zhang</strong>, Michael Feil, Philip Kiely
                  </div>
                  <div class="links">
                    <a
                      href="https://www.baseten.co/blog/day-zero-benchmarks-for-qwen-3-with-sglang-on-baseten"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Private, secure DeepSeek-R1 in production in US & EU data
                    centers
                  </div>
                  <div class="author">
                    Philip Kiely, Amir Haghighat, <strong>Yineng Zhang</strong>
                  </div>
                  <div class="links">
                    <a
                      href="https://www.baseten.co/blog/private-secure-deepseek-r1-in-production-in-us-eu-data-centers"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load
                    Balancer, Faster Structured Outputs
                  </div>
                  <div class="author">
                    Byron Hsu, Ke Bao, Lianmin Zheng,
                    <strong>Yineng Zhang</strong>, Ziyi Xu
                  </div>
                  <div class="links">
                    <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    SGLang: Fast Serving Framework for Large Language and
                    Vision-Language Models on AMD Instinct GPUs
                  </div>
                  <div class="author">
                    Michael Zhang, Hai Xiao, Hui Liu,
                    <strong>Yineng Zhang</strong>
                  </div>
                  <div class="links">
                    <a
                      href="https://rocm.blogs.amd.com/artificial-intelligence/sglang/README.html"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster
                    torch.compile, Multi-Image/Video LLaVA-OneVision
                  </div>
                  <div class="author">
                    Ke Bao, <strong>Yineng Zhang</strong>, Liangsheng Yin,
                    Kaichen Zhang, Bo Li, Ying Sheng
                  </div>
                  <div class="links">
                    <a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Achieving Faster Open-Source Llama3 Serving with SGLang
                    Runtime (vs. TensorRT-LLM, vLLM)
                  </div>
                  <div class="author">
                    Liangsheng Yin, <strong>Yineng Zhang</strong>, Ying Sheng
                  </div>
                  <div class="links">
                    <a href="https://lmsys.org/blog/2024-07-25-sglang-llama3"
                      >Blog</a
                    >
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Meituan Waimai's Practice of Vector Retrieval System Based
                    on GPU (a.k.a. 美团外卖基于 GPU 的向量检索系统实践)
                  </div>
                  <div class="author">到家研发平台, 基础研发平台</div>
                  <div class="adoption">
                    <em
                      ><strong
                        >Yineng Zhang serves as the project lead.</strong
                      ></em
                    >
                  </div>
                  <div class="links">
                    <a
                      href="https://tech.meituan.com/2024/04/11/gpu-vector-retrieval-system-practice.html"
                      >Blog (Chinese)</a
                    >
                  </div>
                </div>
              </div>
            </li>
          </ol>
        </div>

        <h3 id="experience">Experience</h3>

        <div style="padding-right: 10px; padding-left: 20px">
          <p>
            <strong>Together AI</strong> <br />
            Principal AI Researcher <br />
            <em>Lead TGL team</em> <br />
            July 2025 - now
          </p>

          <p>
            <strong>Baseten</strong> <br />
            Lead Software Engineer <br />
            <em>Model Performance Team</em> <br />
            September 2024 - June 2025
          </p>

          <p>
            <strong>Meituan</strong> <br />
            Senior Software Engineer <br />
            <em>Machine Learning Engine Group</em> <br />
            August 2021 - July 2024
          </p>

          <p>
            <strong>Baidu</strong> <br />
            Senior Software Engineer <br />
            <em>Baidu Speech</em> <br />
            June 2020 - August 2021
          </p>

          <p>
            <strong>Stealth Startup</strong> <br />
            Software Engineer <br />
            July 2019 - June 2020
          </p>
        </div>

        <h3 id="education">Education</h3>

        <div style="padding-right: 10px; padding-left: 20px">
          <p>
            <strong>Jiangnan University</strong> <br />
            Bachelor of Engineering <br />
            September 2015 - June 2019
          </p>
        </div>

        <h3 id="publications" style="margin: 2px 0px -15px">Publications</h3>

        <div class="publications">
          <ol class="bibliography">
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    The Measure of All Measures: Quantifying LLM Benchmark
                    Quality
                  </div>
                  <div class="author">
                    Jihan Yao, Peter Jin, Ke Bao, Qiaolin Yu, Khushi Bhardwaj,
                    Chang Su, Jialei Wang, YIKAI ZHU, Sugam Devare, Damon
                    Mosk-Aoyama, Zhen Dong, Venkat Krishna Srinivasan,
                    <strong>Yineng Zhang</strong>, Oleksii Kuchaiev, Jiantao
                    Jiao, Banghua Zhu
                  </div>
                  <div class="adoption">
                    <em
                      ><strong>
                        NeurIPS'25 LLM Evaluations Workshop <br />
                        Oral</strong
                      ></em
                    >
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    Locality-aware Fair Scheduling in LLM Serving
                  </div>
                  <div class="author">
                    Shiyi Cao*, Yichuan Wang*, Ziming Mao, Pin-Lun Hsu,
                    Liangsheng Yin, Tian Xia, Dacheng Li, Shu Liu,
                    <strong>Yineng Zhang</strong>, Yang Zhou, Ying Sheng, Joseph
                    Gonzalez, Ion Stoica
                  </div>
                  <div>*indicates equal contribution</div>
                  <div class="links">
                    <a href="https://www.arxiv.org/abs/2501.14312">Paper</a>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    FlashInfer: Efficient and Customizable Attention Engine for
                    LLM Inference Serving
                  </div>
                  <div class="author">
                    Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin,
                    <strong>Yineng Zhang</strong>, Stephanie Wang, Tianqi Chen,
                    Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze
                  </div>
                  <div class="adoption">
                    <em
                      ><strong>
                        MLSys 2025 Best Paper Award <br />
                        <a href="https://github.com/flashinfer-ai/flashinfer"
                          >FlashInfer</a
                        >
                        has been adopted by SGLang, vLLM and MLC LLM.</strong
                      ></em
                    >
                  </div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2501.01005">Paper</a>
                  </div>
                </div>
              </div>
            </li>
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div class="title">
                    QQQ: Quality Quattuor-Bit Quantization for Large Language
                    Models
                  </div>
                  <div class="author">
                    Ying Zhang, Peng Zhang, Mincong Huang, Jingyang Xiang, Yujie
                    Wang, Chao Wang, <strong>Yineng Zhang</strong>, Lei Yu,
                    Chuan Liu, Wei Lin
                  </div>
                  <div class="adoption">
                    <em
                      ><strong> ICLR 2025 Workshop SCI-FM <br /></strong
                      ><strong
                        >QQQ has been adopted by
                        <a href="https://github.com/vllm-project/vllm/pull/5218"
                          >vLLM</a
                        >
                        and
                        <a href="https://github.com/pytorch/ao/pull/1113"
                          >torchao</a
                        >.</strong
                      ></em
                    >
                  </div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2406.09904">Paper</a>
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>
          </ol>
        </div>

        <h3 id="news" style="margin: 2px 0px -15px">News</h3>

        <div class="publications">
          <ol class="bibliography">
            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div>
                    May 8, 2025: FlashInfer has been selected for the Best Paper
                    Award at MLSys 2025!
                  </div>
                  <div class="links">
                    <a href="https://mlsys.org/virtual/2025/poster/3259"
                      >FlashInfer: Efficient and Customizable Attention Engine
                      for LLM Inference Serving
                    </a>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div>
                    Feb 11, 2025: FlashInfer has been accepted at MLSys 2025.
                  </div>
                  <div class="links">
                    <a href="https://arxiv.org/abs/2501.01005"
                      >FlashInfer: Efficient and Customizable Attention Engine
                      for LLM Inference Serving
                    </a>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div>
                    Dec 26, 2024: The SGLang and DeepSeek teams worked together
                    to get DeepSeek V3 FP8 running on NVIDIA and AMD GPU from
                    day one.
                  </div>
                  <div class="links">
                    <a
                      href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"
                      >SGLang v0.4.1 Release
                    </a>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div>
                    Nov 25, 2024: SGLang has been the dominant large language
                    model inference engine at AMD.
                  </div>
                  <div class="links">
                    <a
                      href="https://community.amd.com/t5/ai/unlocking-new-horizons-in-ai-and-hpc-with-the-release-of-amd/ba-p/726434"
                      >AMD Community Announcement
                    </a>
                  </div>
                </div>
              </div>
            </li>

            <li>
              <div class="pub-row">
                <div
                  class="col-sm-12"
                  style="
                    position: relative;
                    padding-right: 10px;
                    padding-left: 20px;
                  "
                >
                  <div>
                    Aug 24, 2024: SGLang has been adopted by xAI to power the
                    Grok-2 model's inference.
                  </div>
                  <div class="links">
                    <a href="https://x.com/xai/status/1827151960300007434"
                      >xAI Announcement
                    </a>
                  </div>
                </div>
              </div>
            </li>

            <div style="line-height: 50%">
              <br />
            </div>
          </ol>
        </div>

        <br />
      </section>
      <footer></footer>
    </div>
    <script src="/%20/assets/js/scale.fix.js"></script>

    <script>
      (function (i, s, o, g, r, a, m) {
        i["GoogleAnalyticsObject"] = r;
        (i[r] =
          i[r] ||
          function () {
            (i[r].q = i[r].q || []).push(arguments);
          }),
          (i[r].l = 1 * new Date());
        (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m);
      })(
        window,
        document,
        "script",
        "https://www.google-analytics.com/analytics.js",
        "ga",
      );
      ga("create", "UA-102621545-1", "auto");
      ga("send", "pageview");
    </script>
  </body>
</html>
